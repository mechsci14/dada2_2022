---
title: "dada2 lecture"
author: "Madelyn Houser and Vicki Hertzberg"
date: "2/16/2022"
output:
  html_document:
    toc: true
  word_document:
    toc: true
---

The microbiome, that "new organ" comprising the trillions of microbes that live in and on us, is thought to play a significant role in health and disease. We are going to talk about how to identify bacteria isolated from a sample by sequencing a portion of the genetic material present, lining these sequences up into counts of organisms, then using these data to describe the microbes present in numeric or graphical fashion.
  
  
Please note that we have very liberally borrowed from Ben Callahan's dada2 tutorial in creating this lesson.
https://benjjneb.github.io/dada2/tutorial_1_8.html
  
  
# Introduction
  
"Microbiome" - a microbial community occupying a well-defined habitat as well as its "theatre of activity." A dynamic and interactive micro-ecosystem.
  
'theatre of activity' includes microbial genetic elements, structural elements, metabolites, and surrounding micro-environmental conditions

"Microbiota" - all living members forming the microbiome (often includes bacteria, fungi, viruses)
  
  
Microbiota are deeply integrated into the physiology of their host. They impact metabolism, host gene expression, host endocrine activity, immune responses, etc. They are biologically active themselves and produce numerous metabolites which can have direct effects on host processes (bile acids, short-chain fatty acids, hormones, neurotransmitters like serotonin and GABA, etc.)
  
Because of this interconnectivity with host physiology, the microbiome has been implicated in a wide range of conditions impacting human health, including tissue-specific infection and inflammatory conditions, obesity and metabolic disorders, autoimmunity and allergic diseases, pain, psychiatric conditions, and neurological disorders. The microbiome can also impact drug metabolism.
  
The goal of most microbiome research is to identify which taxa (kinds of microbes) are associated with particular conditions related to health or disease in hopes that this can serve as a biomarker for at-risk individuals and/or can be modulated to prevent the development of disease or ameliorate the symptoms.
  
To do this, we start by sequencing microbial DNA. There are different techniques to do this:
Amplicon sequencing - specific regions of microbial genomes are amplified using PCR (hence "amplicon") and then sequenced. These are typically highly conserved small subunits of the gene that encodes for ribosomal RNA, in particular the 5S, 16S, 18Sm or 23S regions (depending on the type of organism being targeted; 16S is most commonly used to target bacteria). These genes contain regions of highly conserved DNA where primers can bind as well as variable regions that distinguish different taxa.For instance, the 16S rRNA unit, about 1500 base pairs (bp) in length, has 9 hypervariable regions. For fungi, the ITS unit serves this purpose. You can design an experiment so that you target one of those regions, or you can sequence the whole 16S region. Your choice of primers (and budget) will determine how much you sequence. The taxonomic resolution of amplicon sequencing can be limited, but it is relatively inexpensive, requires very little sample, and there are well-established analytics tools. 

Alternatively you can perform whole genome sequencing (WGS), sometimes also called "shotgun" sequencing, which sequences all DNA in a sample. This can provide information about all kinds of microbes in a sample with excellent taxonomic resolution. WGS is about 10 times more expensive then 16S sequencing, though, requires a larger amount of sample, and involves more complex bioinformatics.
  
  
# 16S Sequencing

Sequences determined by Illumina MiSeq (a particular sequencing instrument) protocol
- Should get nearly complete overlap of forward and reverse reads.
- These can be merged to form a high quality consensus base call at each position.
- Then classify the merged reads.

Bonus info:
*Universal PCR primers are used to amplify the V4 hypervariable region of the 16S gene. These will produce 288-290 bp amplicons covering the V4 region.*
  
*Universal primers:*
*- Forward (515F): GTGCCAGCMGCCGCGGTAA*
*- Reverse (805R): GGACTACHVGGGTWTCTAA*
  
*Wait! I thought that the only letters allowed in the sequences are A, C, G, or T. Yet I see M, H, V, and W in those primer sequences. What's up with that?*
*It turns out that the Illumina is smart enough to know what it doesn't know. So if it knows the base, it calls it that way. OTOH, if it can only resolve the base call down to, say, the choice of 2 bases, it uses these other letters to designate that situation. These are called the IUPAC Ambiguity Codes, and you can see them here: http://droog.gs.washington.edu/parc/images/iupac.html. There is another IUPAC code not included on the table, and that is "-", which is used to indicate a gap, an often useful indicator for alignments.*
  
More Bonus info:
*In the files produced by the sequencer, every read represents an independent copy of source DNA in your sample. When the target material is sequenced, there are two main considerations: sequencing *breadth* and sequencing *depth*.*

- *Breadth refers to the extent to which you sequence the entire genome present. You want to be sure that you have sequence information for all areas of the target.*
- *Depth refers to how many reads on average cover each base pair of your target. This is also sometimes referred to as "coverage."*

- *In 16s rRNA amplicon sequencing, the primers that you use determine breadth.* 
- *If you don't have sufficient depth you may end up with incomplete or inconclusive results. OTOH, oversequencing raises costs.* 
  
    
## Sequencing File Format (Illumina FASTQ)

File Naming Conventions:
- NA10831_ATCACG_L002_R1_001.fastq.gz 
- FA1_S1_L001_R1_001.fastq.gz 
- Sample_Barcode/Index_Lane_Read#_Set#.fastq.gz
  
| 4 lines per read as follows: |
|------------------------------|
| \@sequence_id                |
| sequence                     |
| \+                           |
| quality                      |
|------------------------------|
  
Example:
  
- Line1: \@M00763:36:000000000-A8T0A:1:1101:14740:1627 1:N:0:1
  
- Line2: CCTACGGGAGGCAGCAGTGGGGAATATTGCACAATGGGGGAAACCCTGATGC AGCGACGCCGCGTGAGTGAAGAAGTATCTCGGTATGTAAAGCTCTATCAGCA GGAAAGATAATGACGGTACCTGACTAAGAAGCCCCGGCTAACTACGTGCCAG CAGCCGCGGTAATACGTAGGGGGCAAGCGTTATCCGGATTTACTGGGTGTAA AGGGAGCGTAGACGGCAGCGCAAGTCTGGAGTGAAATGCCGGGGCCCAACCC 
CGGCCCTGCTTTGGAACCCGTCCCGCTCCAGTGCGGGCGGG
  
- Line3: \+ 
  
- Line 4: 88CCCGDBAF)===CEFFGGGG>GGGGGGCCFGGGGGDFGGGGDCFGGGFED CFG:\@CFCGGGGGGG?FFG9FFFGG9ECEFGGGDFGGGFFEFAFAFFEFECE F\@4AFD85CFFAA?7+C\@FFF<,A?,,,,,,AFFF77BFC,8>,>8D\@FFFF G,ACGGGCFG>\*57;\*6=C58:?<)9?:=:C\*;;\@C?3977\@C7E\*;29>/= +2\*\*)75):17)8\@EE3>D59>)>).)61)4>(6\*+/)\@F63639993??D1 :0)((,((.(.+)(()(-(\*-(-((-,,(.(.)),(-0)))
  
  
Line 1 contains sequence identifiers:
- \@EAS139:136:FC706VJ:2:5:1000:12850 1:N:18:ATCACG 
- \@M00763:36:000000000-A8T0A:1:1101:14740:1627 1:N:0:1
- \@Instrument : Run# : FlowcellID : Lane : Tile : X : Y Read : Filtered : Control# : Barcode/Index
  
Line 2 contains the read sequence
  
Line 4 consists of the Phred scores for each base call. Each character is associated with a value between 0 and 40, and these are called the Quality scores or Q scores. The coding scheme can be found at https://support.illumina.com/help/BaseSpace_OLH_009008/Content/Source/Informatics/BS/QualityScoreEncoding_swBS.htm  
  
Bonus info:
*The relationship is as follows:*
  
\begin{equation}
Q=-10*log(p)
\end{equation}
  
*Or you can solve for p as follows:*
  
\begin{equation}
p=10^{-Q/10}
\end{equation}
  
*You can see from the table that letters have high Quality Scores. Let's look at the Phred score of I, which has a Q value of 40. *
  
|                |
|----------------|
| Q=40           |
| -Q/10 = -4     |
| p = $10^{-4}$|
| p = 0.0001     |
  
  
*In contrast, consider when the Phred score is "+", the Q value is 10, and p = 0.10.*
  
In summary, if you are looking at the .fastq file (which you can open with a text editor), letters are good, numbers are medium, and most anything else is of lesser quality.
  
You want to know this because you may want to trim your sequences. Sometimes when a sequence is read, the first 10 or so sequences are of lesser quality. Also, reads deteriorate in quality towards the end of a sequence. Finally, forward reads are generally of better quality than their corresponding reverse reads.
  
  
  
# Processing Sequencing Data: Overview
  
1. Filter - remove low-quality reads and non-target sequences
2. Trim - prune low-quality ends
3. Assembly - correct overlapping bases
4. Aggregate - combine similar reads
5. Chimeras - remove chimeric reads
  
  
  
# Processing Sequencing Data with the DADA2 Pipeline
  
Today we are going to go through the process of "feeding" a set of Illumina-sequenced paired-end .fastq files into the `dada2` package, with the goal of deriving a *sequence table* in the end. The term "dada" stands for Divisive Amplicon Denoising Algorithm. This R package example uses 4 dada steps, 2 for the forward reads and 2 for the reverse reads, hence "dada2."

## A note about how dada2 works under the hood  
(especially cool for anyone who's been following microbiome research for years and seen the analytical techniques improve)
  
A sequence table is akin to the ubiquitous OTU table, only it is at a much higher resolution. "OTUs," or operational taxonomic units, is a term defined as sets of individual organisms that have been put together in groups according to their DNA similarity. OTUs are often used as proxies for "species". "Similarity" in this context is defined as differing by no more than a fixed dissimilarity threshold, typically set at 3%. But we are going to do better than 97% similarity; we are headed to 100%. 
  
Why isn't 97% good enough?
  
1. It still is low resolution, giving genus level identification at best
2. It tends to have a high false positive rate, with the number of OTUs much greater than actual richness.
3. As you get more samples, the processing time scales up super-linearly.
  
We are using `dada2` because it infers sample sequences exactly, resolving differences of as little as one nucleotide producing Amplicon Sequence Variants (ASVs) (aka exact sequence variants, ESVs) rather than OTUs. Not only does `dada2` provide higher resolution, it also performs better than two other major players in this field: mothur and QIIME (v1). But note that QIIME v2.1 now has a DADA2 step built into it, so it should now compete well with this package. 
  
For the moment, however, we are going to continue to use `dada2`, well, because. Because we are now expert R, RStudio, git and github users, that's why!!!
  
  
  
## Prep
  
### Primer Removal
  
Our "input" into the package is a set of Illumina-sequenced paired-end .fastq files that have been demultiplexed (i.e., split by sample) and *from which the barcodes/adapters/primers have already been removed.*  In this toy dataset all of these barcoode/adapters/primers have been removed. But when it comes to processing your own data, you will have to check this before you proceed.
  
My personal experience with two different labs is that you will receive demultiplexed files with the barcodes removed, *but* the primers have not been removed. You will need to know what the primers are so that you can figure out the lengths. You can trim before hand with a program such as TRIMMOMATIC (http://www.usadellab.org/cms/?page=trimmomatic). This way, when we display the quality plots later you will be seeing the quality of the "meat" of your data, instead of having some distraction with the quality of the sequences including the primers. 
  
You can also trim later by setting an option. I will show you (outside of an R chunk of course, because these data have already had the primers trimmed) how to do that.
  
  
### File Order
  
This pipeline also assumes that if you have paired reads, that the forward and reverse .fastq files contain the reads in matching order.
  
If this is not true, you will need to remedy this before proceeding. Please see the `dada2` FAQ for suggestions: http://benjjneb.github.io/dada2/faq.html#what-if-my-forward-and-reverse-reads-arent-in-matching-order.
  
  
  
  
So let's get started now.

### Setup

Let's check that all is ready.

```{r packages}
library(dada2)
library(ShortRead)
library(ggplot2)
library(DECIPHER)
library(phangorn)
library(phyloseq)
sessionInfo()
```

I have also downloaded the file used in the Mothur MiSeq SOP, as well as two RDP reference files. The Mothur MiSeq files contain data from an experiment in which the V4 region of the 16S rRNA gene in mice feces was sequenced.
    
You will have to change the path in the next chunk to the path to where your files sit. Also if you are on a Windows machine, this will also look different. Let's make sure they are all in the proper place on my machine:

```{r checkfiles}
# Set the path to the data files
#path <- "~/Documents/NRSG_741/MiSeqData/MiSeq_SOP" # for a Mac
path <- "C:/Users/mcrawf4/Documents/NRSG741/Microbiome/miseqsopdata/MiSeq_SOP" # for a PC
list.files(path)
```

OK, I see 38 numbered .fastq files, 2 mock community .fastq files, a few informational files, and the three SILVA V138 files. (The file named "filtered" will be created in another couple of steps, so we are not going to worry about that.)

## Filter and Trim

So now we are ready to use the `dada2` pipeline. We will first read in the names of the .fastq files. Then we will manipulate those names as character variables, using regular expressions to create lists of the forward and reverse read .fastq files in *matched* order.

```{r sortfiles}
# Forward and reverse fastq filenames have format: SAMPLENAME_R1_001.fastq and SAMPLENAME_R2_001.fastq
# Read in the names of the .fastq files

fnFs <- sort(list.files(path, pattern="_R1_001.fastq", full.names=TRUE))
fnRs <- sort(list.files(path, pattern="_R2_001.fastq", full.names=TRUE))

# Extract sample names, assuming filenames have format: SAMPLENAME_XXX.fastq

sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)
  
```

Note: If you are using this workflow with your own data, you will probably need to modify the R chunk above, especially the assignment of sample names to the variable `sample.names`.


### Quality Profiles of the Reads

One of the points that we have repeatedly emphasized in this class is the importance of visualizing your data, and that process is still important with this type of data. Fortunately there is a great quality profile plot that you can generate with just a single command from `dada2`.

```{r qualityF}
# Visualize the quality profile of the first two files containing forward reads

plotQualityProfile(fnFs[1:2])


# You can also generate an aggregate plot of all your forward reads

plotQualityProfile(fnFs, aggregate=TRUE)
```

We see here that the forward reads are really good quality, though the first 15 or so bases and the end of the reads are of lower quality than the rest. 

Let's look at the reverse reads.

```{r qualityR}
# Visualize the quality profile of the first two files containing reverse reads

plotQualityProfile(fnRs[1:2])

# You can also generate an aggregate plot of all your reverse reads

plotQualityProfile(fnRs, aggregate=TRUE)
```

The quality of the reverse reads is subtantially worse, especially toward the end, a common phenomenon with Illumina paired-end sequencing. The dada algorithm incorporates error quality into the model, so it is robust to lower quality sequences, but trimming is still a good idea. It's important to strike a balance between trimming enough to retain only good quality sequences and not trimming so much that you can't get good overlap of your forward and reverse reads. Erring too much on either side will result in much of your sequence data being thrown out (and your analysis potentially failing downstream).
  
### Performing the Filtering and Trimming

We will some typical filtering parameters described below and then set trimming locations based on our quality profiles above.

- `maxN = 0` -- `dada2` requires that there be no N's in a sequence
- `truncQ = 2` -- truncate reads at the first instance of a quality less than or equal to \code{truncQ}#.
- `maxEE` = 2 -- sets the maximum number of expected errors allowed in a read, which is a better filter than simply averaging quality scores.
  
```{r createFiltered}
# Make a directory and filenames for the filtered fastqs
 
# Place filtered files in a filtered/ subdirectory
filt.path <- file.path(path, "filtered")
if(!file_test("-d", filt.path)) dir.create(filt.path)
filtFs <- file.path(filt.path, paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(filt.path, paste0(sample.names, "_R_file.fastq.gz"))

```

Now filter the forward and reverse reads

```{r filter}

out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, trimLeft=c(15, 15), truncLen = c(240, 160),
                     maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
                     compress=TRUE, multithread=FALSE) #On Mac set multithread=TRUE

head(out)
# YOu want to see some filtering but the majority of your reads retained
```
  
Standard filtering parameters as shown here are guidelines, i.e., they are not set in stone. For example, if too few reads are passing the filter, consider relaxing `maxEE`, perhaps especially on the reverse reads, (e.g., `maxEE=c(2,5)`). If you want to speed up downstream computation and have fewer reads pass the filter, consider tightening `maxEE` (e.g., `maxEE=c(1,1)`). For paired-end reads, consider the length of your amplicon when choosing `truncLen` and `trimLeft`, as your reads MUST OVERLAP after truncation in order to merge later.
  
  
## Learn the Error Rates

The `dada2` algorithm uses a parametric error model (`err`), and, of course, every amplicon dataset will have a different set of error rates. The algorithm will learn its error model from the data by alternating estimation of error rates and inference of sample composition until they converge on a jointly consistent solution (like the EM algorithm, if you happen to know that) (and if you don't, it does not matter). As in many machine-learning problems, the algorithm must begin with an initial guess, for which the maximum possible error rates in this data are used (the error rates if only the most abundant sequence is correct and all the rest are errors).

```{r learnErrors}
errF <- learnErrors(filtFs, multithread = FALSE) #On a Mac, set multithread=TRUE
errR <- learnErrors(filtRs, multithread = FALSE) #On a Mac, set multithread=TRUE
```
  
Finally it is always worthwhile to visualize the estimated error rates:

```{r plotErrors}
# Plot the estimated error rates for the Forward reads

plotErrors(errF, nominalQ=TRUE)

# And for the Reverse reads

plotErrors(errR, nominalQ = TRUE)


```

The error rates for each possible type of transition (i.e., A -> C, A -> T, ..., T -> G) are shown. The black points are the observed error rates for each consensus quality score. The black line shows the estimated error rates after convergence. The red line is the error rates expected under the nominal definition of the Q value. You see that the black line (estimated rates) fits the observed rates well, and the error rates drop with increased quality as expected. So all is looking good and we proceed. (If you are working with your own large dataset and the plotted error model does not look like a good fit, you can try increasing the `nbases` parameter to see if the fit improves.)


## Dereplication

You can gain further efficiency by dereplicating the reads, that is, combining all identical sequences so that all you are left with is a list of "unique sequences" and a count of them, defined as the "abundance". Other pipelines can do this, too, to gain efficiency, but `dada2` has an advantage in that it retains a summary of the quality information associated with each unique sequence, developing a consensus quality profile as the average of the positional qualities from the dereplicated reads, which it then uses to inform the error model in the subsequent denoising step.

```{r dereplicate}
# Dereplicate

derepFs <- derepFastq(filtFs, verbose=TRUE)
derepRs <- derepFastq(filtRs, verbose=TRUE)

# Name the derep-class objects by the sample names
names(derepFs) <- sample.names
names(derepRs) <- sample.names
```
  
## Sample Inference

We are now ready to infer the sequence variants in each sample (second dada pass)

```{r sampleinfer}
# First with the Forward reads

dadaFs <- dada(derepFs, err = errF, multithread = FALSE) # On a Mac, set multithread=TRUE

# Then with the Reverse reads

dadaRs <- dada(derepRs, err = errR, multithread = FALSE) # On a Mac, set multithread=TRUE

# Inspect the dada-class objects returned by the dada function

dadaFs[[1]]
dadaRs[[1]]

```
  
  
## Merge Paired Reads

We now merge the forward and reverse reads together to obtain the full denoised sequences. Merging is performed by aligning the denoised forward reads with the reverse-complement of the corresponding denoised reverse reads, and then constructing the merged sequences. By default, merged sequences are only output if the forward and reverse reads overlap by at least 12 bases, and are identical to each other in the overlap region.

```{r mergepaired}

# Merge the denoised forward and reverse reads

mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose=TRUE )

# Inspect the merged data.frame from the first sample

head(mergers[[1]])

```

We now have a `data.frame` object for each sample with the merged `$sequence`, its `$abundance`, and the indices of the merged `$forward` and `$reverse` denoised sequences. Paired reads that did not precisely overlap have been removed by the `mergePairs` function.
  
Most of your reads should successfully merge. If this is not the case, revisit some upstream parameters. In particular, make sure you did not trim too much, removing overlap between reads.
 
  
## Sequence Table Construction

We will now construct the sequence table.

```{r seqtable}

# Construct sequence table

seqtab <- makeSequenceTable(mergers)

# Consider the table

dim(seqtab)
class(seqtab)

# Inspect the distribution of sequence lengths

table(nchar(getSequences(seqtab)))


```

We see that the sequence table is a `matrix` with rows corresponding to and named by the samples, and columns corresponding to and named by the sequence variants (ASVs). We also see that the lengths of all of the sequences fall in the range expected for V4 amplicons.
  
If working with your own data you may find sequences that are much longer or much shorter than expected. These may be the result of non-specific priming, and you should consider removing them. Use the command `seqtab2 <- seqtab[,nchar(colnames(seqtab)) %in% seq(250, 256)]`.
  
  
## Remove Chimeras

So far we have let the `dada` function remove substitution errors and indel (insertion/deletion) errors, but chimeras remain. The accuracy of the sequences after denoising makes chimera identification easier than if we had done that earlier with "fuzzier" sequences because all chimeric sequences now can be exactly reconstructed as a bimera (two-parent chimera) from more abundant sequences.

```{r chimeras}

# Remove chimeric sequences

seqtab.nochim <- removeBimeraDenovo(seqtab, method = "consensus", multithread=FALSE, verbose=TRUE) # On a Mac, set multithread=TRUE
dim(seqtab.nochim)

sum(seqtab.nochim)/sum(seqtab)

# Save table (In a large dataset, the above steps are time- and memory-intensive; save the table now, and if you needed to come back later and make changes to later steps in this process, you won't have to redo all the long steps)
# You will need to replace the path below with the path where you want to save the file on your computer
saveRDS(seqtab.nochim, "C:/Users/mcrawf4/Documents/NRSG741/Microbiome/seqtab.nochim.rds")
```

The fraction of chimeras can be substantial. In this example, chimeras account for `r ncol(seqtab)-ncol(seqtab.nochim)` out of `r ncol(seqtab)` unique sequence variants, or about `r round(((ncol(seqtab)-ncol(seqtab.nochim))/ncol(seqtab))*100)`% of them, but these variants account for only about
`r 100-(round(sum(seqtab.nochim)/sum(seqtab)*100))`% of the total sequence reads.
  
Most of the _reads_ should remain after chimera removal, although it is not uncommon for a majority of _sequence variants_ to be removed. If most of your reads are removed as chimeric, you may need to revisit upstream processing. In almost all cases this is caused by primer sequences with ambiguous nucleotides that were not removed prior to beginning the DADA2 pipeline.
  
  
## Track Reads through the Pipeline

```{r track}

getN <- function(x) sum(getUniques(x))
pctSurv <- rowSums(seqtab.nochim)*100/out[,1]
track <- cbind(out, sapply(dadaFs, getN), sapply(mergers, getN), rowSums(seqtab), rowSums(seqtab.nochim), pctSurv)
colnames(track) <- c("input", "filtered", "denoised", "merged", "tabled", "nonchimeric", "% passing")
rownames(track) <- sample.names
head(track)

```
  
Ideally, there should be no step (other than possibly filtering depending on how stringent you set your criteria) in which a majority of reads are lost. If this is the case, go back and adjust parameters. This is also a good place to check and see if any samples are of substantially lower quality (have far fewer reads passing) than the rest of the samples. If so, you may need to exclude those samples from downstream analysis (we'll see how to do that in the next lesson). If things look good, congratulations! Proceed.
  
  
## Assign Taxonomy

Most people want to know the names of the organisms associated with the sequence variants, and so we want to classify them taxonomically. The package will use a classifier for this purpose, taking a set of sequences and a training set of reference sequences with known taxonomy, and outputs taxonomic assignments with at least `minBoot` bootstrap confidence. 

There are many training sets to use. GreenGenes is one such set, but it has not been updated in years. UNITED ITS and the Ribosomal Database Project (RDP) are others, the former being used for fungi. We are going to use a training set from SILVA. You should have downloaded that earlier and it should be sitting in the same folder as the original forward and reverse read files. Replace the paths in the code chunks below with the location of the files on your computer.

```{r assigntax}  
# First initialize random number generator for reproducibility
set.seed(02142022)

# Assign taxonomy
#taxa <- assignTaxonomy(seqtab.nochim, "~/Documents/NRSG_741/MiSeqData/MiSeq_SOP/silva_nr99_v138_wSpecies_train_set.fa.gz", multithread = TRUE) # for a Mac
taxa <- assignTaxonomy(seqtab.nochim, "C:/Users/mcrawf4/Documents/NRSG741/Microbiome/miseqsopdata/MiSeq_SOP/silva_nr99_v138_wSpecies_train_set.fa.gz") # for a PC
unname(head(taxa))
```

### Species Assignment

We can also use the SILVA species assignment dataset to do exactly that - assign species.

```{r assignspecies}
# Assign species
#taxa <- addSpecies(taxa, "~/Documents/NRSG_741/MiSeqData/MiSeq_SOP/silva_species_assignment_v132.fa.gz") # for a Mac
taxa <- addSpecies(taxa, "C:/Users/mcrawf4/Documents/NRSG741/Microbiome/miseqsopdata/MiSeq_SOP/silva_species_assignment_v138.fa.gz") # for a PC
```
  
Inspect the taxonomic assignments:

```{r seeTaxa}

taxa.print <- taxa #Removing sequence rownames for display only
rownames (taxa.print) <- NULL
head(taxa.print)

```
  
  
### Alternative taxonomy assignment approach
  
Here is an alternative taxonomic classification method available via the `DECIPHER` package from Bioconductor. You will need to download a new classifier dataset from [http://www2.decipher.codes/Downloads.html], using the SILVA SSU r138 (modified) link near the bottom of the page.

```{r DECIPHER}

dna <- DNAStringSet(getSequences(seqtab.nochim)) #Create a DNAStringSet from the ASVs

load(file.path("C:/Users/mcrawf4/Documents/NRSG741/Microbiome/miseqsopdata/MiSeq_SOP/SILVA_SSU_r138_2019.RData")) #CHANGE TO WHERE YOU HAVE STORED THIS DATASET
ids <- IdTaxa(dna, trainingSet, strand="top", processors = NULL, verbose = FALSE)
ranks <- c("domain", "phylum", "class", "order", "family", "genus", "species") # ranks of interest
# Convert the output object of class "Taxa" to a matrix analogous to the output from assignTaxonomy
taxid <- t(sapply(ids, function(x) {
        m <- match(ranks, x$rank)
        taxa <- x$taxon[m]
        taxa[startsWith(taxa, "unclassified_")] <- NA
        taxa
}))
colnames(taxid) <- ranks; rownames(taxid) <- getSequences(seqtab.nochim)

# If you want to use these new taxonomic assignments, set taxa <- taxid
#taxa <- taxid   # Don't do this if you want to use the original taxonomic assignments from the naive Bayes classifier employed by the dada2 `assignTaxonomy` and `assignSpecies` functions

```
  
  
### Evaluate Accuracy

In addition to the MiSeq_SOP files, a "mock community" was also analyzed in this experiment, a mixture of 20 known strains of microbes. This serves as a control, verifying that we can identify the strains we know are in the mock community. Reference sequences are provided in the downloaded zip archive. Let's see how the sequences inferred by `dada2` compare to the expected composition of the community.

```{r mock}

# Evaluate DADA2's accuracy on the mock community

unqs.mock <- seqtab.nochim["Mock",]
unqs.mock <- sort(unqs.mock[unqs.mock>0], decreasing=TRUE) #Drop ASVs absent in the Mock Community
cat("DADA2 inferred", length(unqs.mock), "sample sequences present in the Mock Community. \n")

mock.ref <- getSequences(file.path(path, "HMP_MOCK.v35.fasta"))
match.ref <- sum(sapply(names(unqs.mock), function(x)any(grepl(x, mock.ref))))
cat("Of those,", sum(match.ref), "were exact matches to the expected reference sequences. \n")

```
  
  
## Construct a Phylogenetic Tree

Phylogenetic relatedness is often used to inform downstream analyses, particularly the calculation of phylogeny-aware distances between microbial communities, including the weighted and unweighted UniFrac distances. We can use the reference-free `dada2` sequence inference to construct the phylogenetic tree by relating the inferred sequence variants *de novo*. 

```{r treeprep}

seqs <- getSequences(seqtab.nochim)

# This next command will allow propagation of sequence names to the tip labels of the tree
names(seqs) <- seqs
alignment <- AlignSeqs(DNAStringSet(seqs), anchor=NA)
```

Now that the sequences are aligned, we can use the `phanghorn` package to construct the tree. 

```{r phytree}
# Construct the tree
phang.align <- phyDat(as(alignment, "matrix"), type="DNA")
dm <- dist.ml(phang.align)
treeNJ <- NJ(dm) # Tip order will not equal sequence order
fit <- pml(treeNJ, data=phang.align)

## negative edges length changed to 0.

fitGTR <- update(fit, k=4, inv=0.2)
fitGTR <- optim.pml(fitGTR, model="GTR", optInv=TRUE, optGamma=TRUE, 
                    rearrangement = "stochastic", control=pml.control(trace=0))
detach("package:phangorn", unload=TRUE)

```
  
  
  
## Handoff to `phyloseq`

Our sequence data processing is just about done. Now we hand off the processed data to the `phyloseq` package for analysis. This package requires three items: the sequence table, the taxonomy table, and data about the samples. You can also include a phylogenetic tree, although it is not required. The sequence table and taxonomy table are directly available at the end of the `dada2`run, and you can import the latter as a .csv file. In the case of the data that are considered here, we can derive the sex (S), mouse subject number (X), and day post-weaning (Y) directly from the file name, which has the form SXDY.

```{r samdf}
# Create a data frame for the sample data
samples.out <- rownames(seqtab.nochim)

# Create subject, sex, and day variables
subject <- sapply(strsplit(samples.out, "D"), `[`, 1)
sex <- substr(subject,1,1)
subject <- substr(subject, 2, 999)
day <- as.integer(sapply(strsplit(samples.out, "D"), `[`, 2))

# Combine into dataframe
samdf <- data.frame(Subject = subject, Sex = sex, Day = day)

#Create indicator of early or late day of post-weaning
samdf$When <- "Early"
samdf$When[samdf$Day > 100] <- "Late"

# Assign rownames to the dataframe == these will be the same as the rownames of the "OTUtable"
rownames(samdf) <- samples.out
```

Now that we have our sample data, let's create the phyloseq object.

```{r phyloseq}
# Create phyloseq object
ps <- phyloseq(otu_table(seqtab.nochim, taxa_are_rows=FALSE), 
               sample_data(samdf),
               tax_table(taxa),
               phy_tree(fitGTR$tree))

#Remove mock sample
ps <- prune_samples(sample_names(ps) != "Mock", ps) 

# Describe the resulting phyloseq object
ps

# Save phyloseq object
saveRDS(ps, "C:/Users/mcrawf4/Documents/NRSG741/Microbiome/phyloseqobject.rds")
```
  
## `Phyloseq` output examples
  
So we are now ready to use `phyloseq`. I will show you a few things you can do with these data. In our next session I will show you much much more.
  
```{r alpha, fig.width=2.5, fig.height=4, message=FALSE}
# Plot number of taxa
plot_richness(ps, x="When", measures = c("Observed"), color = "When")  +
  geom_point(size=2) +
  theme_bw() +
  ylab("Number of Taxa") +
  xlab("") +
  theme(legend.position = "none")
```
  
```{r beta, message=FALSE}
# Ordinate with Bray-Curtis
ord.nmds.bray <- ordinate(ps, method="NMDS", distance="bray")
plot_ordination(ps, ord.nmds.bray, color="When", title="Bray NMDS") +
  geom_point(size=2)
```
  
```{r top20}
# Create bar plots for most abundant ASVs
top20 <- names(sort(taxa_sums(ps), decreasing=TRUE))[1:20]
ps.top20 <- transform_sample_counts(ps, function(ASV) ASV/sum(ASV))
ps.top20 <- prune_taxa(top20, ps.top20)
plot_bar(ps.top20, x="Day", fill="Family") + facet_wrap(~When, scales="free_x")
```
  
Next time we'll get much more in-depth with the capabilities of `phyloseq`.